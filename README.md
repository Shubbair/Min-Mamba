# Min Mamba
simple implementation of Mamba model with tensorflow
<img src="assets/mamba.png"/>

## What is Mamba ? 
Mamba at it's core is a recurrent neural network architecture, that outperforms Transformers with faster inference and improved handling of long sequences of length up to 1 million

## Why Mamba ? 
Mamba is 5x faster throughput than Transformers and scales linearly instead of quadratically with the length of the sequence.

the problem of Attention does not compress the context at all ,it gives the model full access to the history

## Why called Mamba ?
They call it Mamba because the build on work called S4 models to create ‚Äúselective structured state space sequence models‚Äù , so it has alot of sss...like snake sound üêç


